{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac43507",
   "metadata": {},
   "source": [
    "### Project Implementation Introduction\n",
    "\n",
    "<p>\n",
    "    Introductory Paragraph\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7be1adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " \n",
       "    <style>\n",
       "        .output_png {\n",
       "            display: table-cell;\n",
       "            text-align: center;\n",
       "            vertical-align: middle;\n",
       "        }\n",
       "    </style> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python libraries used retrieve dataset path information and time module to estimate model running time\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Turn off warnings completely for the Notebook\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Python libraries used import dataset as a dataframe into the IDE, perform array manipulations\n",
    "# and data visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Python libraries used to preprocess dataset i.e. split the dataset into training set and testing set and also\n",
    "# to standardize the dataset (optimize the dataset)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Supervised Learning models (Classifiers) used to implement the models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Evaluation Metrics used to analyze the performance of the implemented models\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "# SMOTE technique used to eliminate the imbalance in the dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# tensorflow keras Library used to implement the Artificial Neural Network (ANN)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Evaluation Metrics used to analyze the performance of the ANN\n",
    "from tensorflow.keras.metrics import *\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.core.display import HTML as Center\n",
    "\n",
    "Center(\"\"\" \n",
    "    <style>\n",
    "        .output_png {\n",
    "            display: table-cell;\n",
    "            text-align: center;\n",
    "            vertical-align: middle;\n",
    "        }\n",
    "    </style> \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc4f00",
   "metadata": {},
   "source": [
    "#### Variable Delaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting to enable IDE and pandas to display up 200 records at a time for data records less than 200 records\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.max_seq_items = 200\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "# the processed dataset for analysis will be store in this dictionary\n",
    "analysis_df_dict = dict()\n",
    "\n",
    "# All result from the analysis will be store in this dictionary \n",
    "analysis_results = dict()\n",
    "\n",
    "# The dependent variable attribute is store in this variable\n",
    "target_variable = 'traffic_type'\n",
    "\n",
    "# these are the key for starting the training and test dataset in the analysis_df_dict dictionary variable\n",
    "x_train_key = \"x_train\"\n",
    "x_test_key = \"x_test\"\n",
    "y_train_key = \"y_train\"\n",
    "y_test_key = \"y_test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44babb8c",
   "metadata": {},
   "source": [
    "### Implemented Objects Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is used to create an instance of artificial neural network (ANN). it uses tensorflow keras Sequential modules to build\n",
    "# the model and uses tensorflow keras layer Dense Module to create the input layer, the hidden layer and the output layer.\n",
    "# The KerasClassifier module is the wrapper that allow the use of sklearn GridSearchCV module to perform hyper-parameter \n",
    "# tuning\n",
    "\n",
    "class ArtificailNeuralNetworkClassifier:\n",
    "    __loss = None\n",
    "    __optimizer = None\n",
    "    __metrics = list()\n",
    "    __epochs = None\n",
    "    __batch_size  = None\n",
    "    __input_dim = None\n",
    "    __relu = \"relu\"\n",
    "    __sigmoid=\"sigmoid\"\n",
    "    __softmax=\"softmax\"\n",
    "    __kernel_initializer=\"random_uniform\"\n",
    "    \n",
    "    # Constructor function\n",
    "    def __init__(self, input_dim, loss, optimizer, metrics, epochs = 10, batch_size=100):\n",
    "        self.__loss = loss\n",
    "        self.__optimizer = optimizer\n",
    "        self.__metrics = metrics\n",
    "        self.__epochs = epochs\n",
    "        self.__batch_size = batch_size\n",
    "        self.__input_dim = input_dim\n",
    "    \n",
    "    # neural network model build function\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.__input_dim, input_dim=self.__input_dim, activation=self.__relu, kernel_initializer=self.__kernel_initializer))\n",
    "        model.add(Dense(1,activation=self.__sigmoid,kernel_initializer=self.__kernel_initializer))\n",
    "        model.add(Dense(2,activation=self.__softmax))    \n",
    "        model.compile(\n",
    "            loss = self.__loss,\n",
    "            optimizer = self.__optimizer,\n",
    "            metrics = self.__metrics\n",
    "        )\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    # build and compile the ANN model and wrapped using KerasClassifier module\n",
    "    def get_model(self):\n",
    "        return KerasClassifier(lambda: self.build_model(), epochs=self.__epochs, batch_size=self.__batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d0b53",
   "metadata": {},
   "source": [
    "#### Reusable Function Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to plot pie charting showing the class distribution by class in percentage(%) \n",
    "# in the dependent variable\n",
    "\n",
    "def show_pie_chart_target_variable(df, labels, colors, title):\n",
    "    class_counts = [df[target_variable].value_counts()[0], df[target_variable].value_counts()[1]]\n",
    "    count = df[target_variable].value_counts().to_frame().sort_index()\n",
    "    plt.pie(class_counts, labels=labels, autopct='%1.1f%%', colors=[colors[c] for c in count.index])\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to plot bar charting showing the class distribution by class  using frequency count \n",
    "# in the dependent variable\n",
    "\n",
    "def show_bar_chart_target_variable(df, target, labels, colors, title):\n",
    "    ax = sns.countplot(x=target, data=df, palette=colors)\n",
    "    ax.bar_label(container=ax.containers[0], labels=labels)\n",
    "    ax.set(title=title)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd1ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used to remove punctuation mark in the found to have been appended to the dataset dependent variable\n",
    "# values\n",
    "\n",
    "def remove_punctuation_from_value(x):\n",
    "    return x.split('.')[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to split the features as categorical variable and continuous variables. The concept here is that\n",
    "# features with pandas' object datatype and integer with less than 10 unique different value were label categorical \n",
    "# variables. The rest features that was not in previous category was label are market as continuous variable and this \n",
    "# return a tuple\n",
    "\n",
    "def split_dataframe_into_continuious_categorical_column_names(df):\n",
    "    \n",
    "    categorical_column_names = list(set(df.select_dtypes(\"object\").columns))\n",
    "    numeric_column_names = list(set(df.select_dtypes(\"number\").columns))\n",
    "    \n",
    "    categorical_column_names = categorical_column_names if isinstance(categorical_column_names, list) else []\n",
    "    continuous_column_names = list()\n",
    "    \n",
    "    for column_name in numeric_column_names:\n",
    "        value_count = df[column_name].nunique()\n",
    "        if value_count < 10:\n",
    "            categorical_column_names.append(column_name)\n",
    "        else:\n",
    "            continuous_column_names.append(column_name)\n",
    "        \n",
    "    return categorical_column_names, continuous_column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to retrieve the features names of column whose datatype is of the object(string) datatype\n",
    "\n",
    "def get_object_data_type_column_names(df):\n",
    "    categorical_cols = df.select_dtypes(\"object\").columns\n",
    "    return categorical_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11981680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used \n",
    "\n",
    "def get_continuous_feature(df, feature_column_list):\n",
    "    features = list()\n",
    "    for feature in feature_column_list:\n",
    "        if df[feature].dtype == 'float64':\n",
    "            features.append(feature)\n",
    "            \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bar_plots_for_all_categorical_plots(df, feature_column_list):\n",
    "    \n",
    "    fig, axes = plt.subplots(len(feature_column_list), 1,tight_layout=False)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    for ax, feature in zip(axes, feature_column_list):\n",
    "        print(\"Summary for {} feature value counts\".format(feature.upper()))\n",
    "        print()\n",
    "        print(df[feature].value_counts())\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        title = \"This is title for {} bar chart\".format(feature.upper())\n",
    "        plt.figure(figsize=(25,10))\n",
    "        ax = df[feature].value_counts().plot(kind=\"bar\")\n",
    "        ax.set(title=title)\n",
    "        \n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_histogram_plots_for_all_continuous_plots(df, feature_column_list):\n",
    "    \n",
    "    feature_columns = get_continuous_feature(df, feature_column_list)\n",
    "    fig, axes = plt.subplots(len(feature_columns), 1,tight_layout=False)\n",
    "    \n",
    "    for ax, feature in zip(axes, feature_columns):\n",
    "        title = \"This is title for {} histogram chart\".format(feature.upper())\n",
    "        plt.figure(figsize=(25,10))\n",
    "        ax = df[feature].value_counts().plot(kind=\"hist\")\n",
    "        ax.set(title=title)\n",
    "        \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d144de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to perform feature mapping which help convert string datatype or pandas object datatype to integer\n",
    "# datastype for columns having the object datatype. This function also invokes the generate_pmap_values() function to\n",
    "# map the string values to integer values\n",
    "\n",
    "def process_feature_mapping(df):\n",
    "    object_column_names = get_object_data_type_column_names(df)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"FEATURE MAPPING FOR OBJECT DATA STRUCTURE IN DATAFRAME\")\n",
    "    print()\n",
    "    for column_name in object_column_names:\n",
    "        print(\"Processing feature mapping for {} feature\".format(column_name))\n",
    "        unique_values = df[column_name].unique()\n",
    "        pmap_values = generate_pmap_values(unique_values)\n",
    "        df[column_name] = df[column_name].map(pmap_values)\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used generate key value pair dictionary data structure, assigning a value incremented by 1 to the list \n",
    "# of string values using a for loop. The initial value for the first item of the list is 0\n",
    "\n",
    "def generate_pmap_values(unique_values):\n",
    "    pmap = {}\n",
    "    initial_value = 0\n",
    "    \n",
    "    for val in unique_values:\n",
    "        pmap[val] = initial_value\n",
    "        initial_value += 1\n",
    "    \n",
    "    return pmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fde393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_NaiveBayes_model_analysis(analysis_df_dict, analysis_results, experiment_type):\n",
    "    model_name = \"NBC\"\n",
    "    model = GaussianNB()\n",
    "    analysis_result = perform_model_analysis(model, analysis_df_dict, model_name)\n",
    "    \n",
    "    if not experiment_type in list(analysis_results.keys()):\n",
    "        analysis_results[experiment_type] = {}\n",
    "        \n",
    "    analysis_results[experiment_type][model_name] = analysis_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f964cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_SVM_model_analysis(analysis_df_dict, analysis_results, experiment_type):\n",
    "    model_name = \"SVM\"\n",
    "    model = svm.SVC(gamma = 'scale')\n",
    "    analysis_result = perform_model_analysis(model, analysis_df_dict, model_name)\n",
    "    \n",
    "    if not experiment_type in list(analysis_results.keys()):\n",
    "        analysis_results[experiment_type] = {}\n",
    "        \n",
    "    analysis_results[experiment_type][model_name] = analysis_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_DecisionTree_model_analysis(analysis_df_dict, analysis_results, experiment_type):\n",
    "    model_name = \"DTC\"\n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
    "    analysis_result = perform_model_analysis(model, analysis_df_dict, model_name)\n",
    "    \n",
    "    if not experiment_type in list(analysis_results.keys()):\n",
    "        analysis_results[experiment_type] = {}\n",
    "        \n",
    "    analysis_results[experiment_type][model_name] = analysis_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c19ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_RFC_model_analysis(analysis_df_dict, analysis_results, experiment_type):\n",
    "    model_name = \"RFC\"\n",
    "    model = RandomForestClassifier(n_estimators=30)\n",
    "    analysis_result = perform_model_analysis(model, analysis_df_dict, model_name)\n",
    "    \n",
    "    if not experiment_type in list(analysis_results.keys()):\n",
    "        analysis_results[experiment_type] = {}\n",
    "        \n",
    "    analysis_results[experiment_type][model_name] = analysis_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ANN_model_analysis(analysis_df_dict, analysis_results, experiment_type):\n",
    "    model_name = \"ANN\"\n",
    "    input_size = analysis_df_dict[x_train_key].shape[1]\n",
    "    ann = ArtificailNeuralNetworkClassifier(input_size, 'categorical_crossentropy', 'adam', [ AUC( name = 'auc') ])\n",
    "    model = ann.get_model()\n",
    "    analysis_result = perform_model_analysis(model, analysis_df_dict, model_name)\n",
    "    \n",
    "    if not experiment_type in list(analysis_results.keys()):\n",
    "        analysis_results[experiment_type] = {}\n",
    "        \n",
    "    analysis_results[experiment_type][model_name] = analysis_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_model_analysis(model, analysis_df_dict, model_name):\n",
    "    \n",
    "    model.fit(analysis_df_dict[x_train_key], analysis_df_dict[y_train_key].values.ravel())\n",
    "        \n",
    "        \n",
    "    model_preditions = model.predict(analysis_df_dict[x_test_key])\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(\"ANALYSIS SUMMARY FOR {} MODEL\".format(model_name))\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(\"CLASSIFICTION REPORT\")\n",
    "    print(\"=========================================================\")\n",
    "    print()\n",
    "    print(classification_report(model_preditions, analysis_df_dict[y_test_key], target_names=[\"Normal\", \"Attack\"]))\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"CONFUSION MATRIX\")\n",
    "    print(\"=========================================================\")\n",
    "    print()\n",
    "    conf_mat = confusion_matrix(analysis_df_dict[y_test_key], model_preditions)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    ax_plot = sns.heatmap(conf_mat, annot=True)\n",
    "    ax_plot.set_title(\"Seaborn Confusion Matrix Plot\")\n",
    "    ax_plot.set_xlabel('Predicted Values')\n",
    "    ax_plot.set_ylabel('Actual Values')\n",
    "    ax_plot.xaxis.set_ticklabels([\"FALSE\", \"TRUE\"])\n",
    "    ax_plot.yaxis.set_ticklabels([\"NEGATIVE\", \"POSITVE\"])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"MODEL PERFORMANCE EVALUATION\")\n",
    "    print(\"=========================================================\")\n",
    "    print()\n",
    "    auc_score = roc_auc_score(analysis_df_dict[y_test_key], model_preditions)\n",
    "    gmean_score = geometric_mean_score(analysis_df_dict[y_test_key], model_preditions)\n",
    "    print(\"AUC SCORE:  {:.4f}\".format(auc_score))\n",
    "    print(\"GEOMETRIC MEAN SCORE:  {:.4f}\".format(gmean_score))\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    result = {\n",
    "        \"AUC\": auc_score,\n",
    "        \"G-MEAN\": gmean_score\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
